![Untitled 001](https://github.com/user-attachments/assets/4ac24d1b-8725-473e-95e1-3ddcd08533a3)

Hey there ğŸ‘‹ Ever wondered how we can make colonoscopies more effective? Well, buckle up, because we're diving into the exciting world of **intelligent colonoscopy**! A quick overview of what we offer for the research community:

- **Discussion forum** â¡ï¸ refer to our [welcome page](https://github.com/orgs/ai4colonoscopy/discussions/1)
  - Join our central hub where you can ask questions, showcase your work, share insights, and access valuable data resources.
- **Must-reading List** â¡ï¸ refer to our [awesome-intelligent-colonoscopy](https://github.com/ai4colonoscopy/awesome-intelligent-colonoscopy)
  - Explore our curated list of essential colonoscopy literature, complete with detailed reading notes to enhance your understanding.
- **AI Express** â¡ï¸ refer to our [ai-paper-express](https://github.com/ai4colonoscopy/awesome-intelligent-colonoscopy/tree/main/ai-paper-express)
  - Discover a platform for sharing the latest AI research papers across various fields, not limited to medical domains.

## ğŸ™‹ News

- [2024/09/01] Create welcome page

## ğŸ¥ Why Should You Care About Colonoscopy?

<p align="center">
  <img src="https://github.com/user-attachments/assets/0fc5d354-e4ab-4fcf-ad7d-a047f53075e7" width="1000px" />
  <br>
  <em> Illustration of a colonoscope inside large intestine (colon). Image credit: https://www.vecteezy.com </em>
</p>


Let's face it - colorectal cancer is a big deal. It's one of the top cancer killers worldwide. But here's the good news: we can often prevent it if we catch it early. That's where colonoscopies come in. They're our best shot at finding and removing those sneaky precancerous polyps before they cause trouble to your body.

But here's the catch - colonoscopies are only as good as the doctor performing them. And let's be real, even the best doctors are human. They get tired, they might miss things sometimes, and some are more experienced than others.

## ğŸ¤– Enter AI: The Game-Changer

<img align="right" src="https://github.com/user-attachments/assets/0de36f70-703b-49f4-ad6b-6c53000dc632" width="475px" />

This is where AI swoops in like a superhero! We're using cutting-edge artificial intelligence to endower colonoscopy a major upgrade. Think of it as giving doctors a pair of super-AI-powered glasses that help them spot things they might otherwise miss.

That's why we're going to explore the critical role of AI in colonoscopy. Here's what AI brings to the table:

- ğŸ” **Improved polyp detection rates**
    - AI is like a tireless assistant, constantly scanning for even the tiniest polyps that human eyes might overlook.
- ğŸ¯ **High sensitivity in distinguishing precancerous polyps**
    - Not all polyps are created equal. AI can be trained to differentiate between the harmless ones and those that could become cancerous, helping doctors prioritize treatment.
- ğŸ–¼ï¸ **Enhanced overall precision of colon evaluation**
    - It's not just about spotting polyps. AI provides a comprehensive view of the colon, helping doctors make more accurate assessments.
- ğŸ˜€ **No added risk to colonoscopy**
    - Here's the best part - all these benefits come with zero additional risk to the patient. It's like getting a free upgrade on your health check!


## ğŸŒ IntelliScope Adventure: Welcome to the World of Intelligent Colonoscopy!

The past few years have been a wild ride in the world of **intelligent colonoscopy** techniques. Let me tell you about one of our proudest achievements:

- `[2024]` **ColonINST & ColonGPT** (On the way! Stay tuned and keep patience!)
    - This year, weâ€™re taking intelligent colonoscopy to the next level, a multimodal world, with three groundbreaking initiatives:
    - ğŸ’¥ Collecting a very-large multimodal dataset ColonINST, featuring 300K+ colonoscopy images, 62 categories, 128K+ GPT-4V-generated medical captions, and 450K+ human-machine dialogues.
    - ğŸ’¥ Developing the first multimodal language model ColonGPT that can handle conversational tasks based on user preferences.
    - ğŸ’¥ Launching a multimodal benchmark to enable fair and rapid comparisons going forward.
- `[MIR-2024]` **Drag&Drop** ([Paper](https://link.springer.com/article/10.1007/s11633-023-1380-5) & [Code](https://github.com/johnson111788/Drag-Drop))
    - Authors: [Yu-Cheng Chou](https://scholar.google.com.tw/citations?user=YVNRBTcAAAAJ&hl) (ğŸ‡ºğŸ‡¸ Johns Hopkins University), [Bowen Li](https://scholar.google.com/citations?user=UfINwO0AAAAJ&hl=en) (ğŸ‡ºğŸ‡¸ Johns Hopkins University), [Deng-Ping Fan](https://scholar.google.com/citations?user=kakwJ5QAAAAJ&hl=en) (ğŸ‡¨ğŸ‡­ ETH ZÃ¼rich), [Alan Yuille](https://scholar.google.com.tw/citations?user=FJ-huxgAAAAJ&hl=en) (ğŸ‡ºğŸ‡¸ Johns Hopkins University), [Zongwei Zhou](https://scholar.google.com.tw/citations?user=JVOeczAAAAAJ&hl=en) (ğŸ‡ºğŸ‡¸ Johns Hopkins University)
    - ğŸ’¥ We introduces the first high-dimensional annotation method, eliminating the need for slice-by-slice labeling.
    - Cuts annotation effort by 87.5% in video polyp detection while maintaining or improving performance.
    - Increases polyp detection precision by 7.8% with the same annotation budget compared to per-pixel methods.
- `[MIR-2022]` **SUN-SEG** ([Paper](https://link.springer.com/article/10.1007/s11633-022-1371-y) & [Code](https://github.com/GewelsJI/VPS))
    - Authors: [Ge-Peng Ji](https://scholar.google.com/citations?user=oaxKYKUAAAAJ&hl=en&authuser=1) (ğŸ‡¦ğŸ‡º Australian National University), [Guobao Xiao](https://jsj.mju.edu.cn/2018/1014/c2248a67381/page.htm) (ğŸ‡¨ğŸ‡³ Minjiang University), [Yu-Cheng Chou](https://scholar.google.com.tw/citations?user=YVNRBTcAAAAJ&hl) (ğŸ‡ºğŸ‡¸ Johns Hopkins University), [Deng-Ping Fan](https://scholar.google.com/citations?user=kakwJ5QAAAAJ&hl=en) (ğŸ‡¨ğŸ‡­ ETH ZÃ¼rich), [Kai Zhao](https://scholar.google.com/citations?hl=en&user=zR5JbZUAAAAJ) (ğŸ‡ºğŸ‡¸ University of California, Los Angeles), [Geng Chen](https://scholar.google.com/citations?user=sJGCnjsAAAAJ&hl=en&authuser=1) (ğŸ‡¦ğŸ‡ª Inception Institute of Artificial Intelligence), [Luc Van Gool](https://scholar.google.com/citations?user=TwMib_QAAAAJ&hl=en&authuser=1) (ğŸ‡¨ğŸ‡­ ETH ZÃ¼rich)
    - ğŸ’¥ We introduce a large-scale and high-quality per-frame annotated VPS dataset, named SUN-SEG, which includes 158,690 frames from the famous [SUN-database](http://amed8k.sundatabase.org/). We extend the expert labels with diverse types, ie, object mask, boundary, scribble, polygon, and visual attribute.
    - Paving the way for future research in colonoscopy video content analysis, with fine-grained categories, annotations.
- `[MICCAI'2021]` **PNS-Net** ([Paper](https://link.springer.com/chapter/10.1007/978-3-030-87193-2_14) & [Code](https://github.com/GewelsJI/PNS-Net))
    - Authors: [Ge-Peng Ji](https://scholar.google.com/citations?user=oaxKYKUAAAAJ&hl=en&authuser=1) (ğŸ‡¦ğŸ‡ª Inception Institute of Artificial Intelligence), [Yu-Cheng Chou](https://scholar.google.com.tw/citations?user=YVNRBTcAAAAJ&hl) (ğŸ‡¨ğŸ‡³ Wuhan University), [Deng-Ping Fan](https://scholar.google.com/citations?user=kakwJ5QAAAAJ&hl=en) (ğŸ‡¦ğŸ‡ª Inception Institute of Artificial Intelligence), [Geng Chen](https://scholar.google.com/citations?user=sJGCnjsAAAAJ&hl=en&authuser=1) (ğŸ‡¦ğŸ‡ª Inception Institute of Artificial Intelligence), [Huazhu Fu](https://scholar.google.com/citations?user=jCvUBYMAAAAJ&hl=en&authuser=1) (ğŸ‡¦ğŸ‡ª Inception Institute of Artificial Intelligence), [Debesh Jha](https://scholar.google.com/citations?user=mMTyE68AAAAJ&hl=en) (ğŸ‡³ğŸ‡´ SimulaMet), [Ling Shao](https://scholar.google.com/citations?user=z84rLjoAAAAJ&hl=en&authuser=1) (ğŸ‡¦ğŸ‡ª Inception Institute of Artificial Intelligence)
    - ğŸ’¥ This super-efficient model (~140fps) is designed for video-level polyp segmentation.
    - We received MICCAI Student Travel Award ([link](https://www.miccai2021.org/en/MICCAI-2021-TRAVEL-AWARDS.html))
    - It's already racked up over 130 citations ([Google Scholar](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=oaxKYKUAAAAJ&authuser=1&citation_for_view=oaxKYKUAAAAJ:MXK_kJrjxJIC)).
- `[MICCAI'2020]` **PraNet** ([Paper](https://link.springer.com/chapter/10.1007/978-3-030-59725-2_26) & [Code](https://github.com/DengPingFan/PraNet))
    - Authors: [Deng-Ping Fan](https://scholar.google.com/citations?user=kakwJ5QAAAAJ&hl=en) (ğŸ‡¦ğŸ‡ª Inception Institute of Artificial Intelligence), [Ge-Peng Ji](https://scholar.google.com/citations?user=oaxKYKUAAAAJ&hl=en&authuser=1) (ğŸ‡¨ğŸ‡³ Wuhan University), [Tao Zhou](https://scholar.google.com/citations?user=LPPsgWUAAAAJ&hl=en) (ğŸ‡¦ğŸ‡ª Inception Institute of Artificial Intelligence), [Geng Chen](https://scholar.google.com/citations?user=sJGCnjsAAAAJ&hl=en) (ğŸ‡¦ğŸ‡ª Inception Institute of Artificial Intelligence), [Huazhu Fu](https://scholar.google.com/citations?user=jCvUBYMAAAAJ&hl=en&authuser=1) (ğŸ‡¦ğŸ‡ª Inception Institute of Artificial Intelligence), [Jianbing Shen](https://scholar.google.com/citations?user=_Q3NTToAAAAJ&hl=en) (ğŸ‡¦ğŸ‡ª Inception Institute of Artificial Intelligence), [Ling Shao](https://scholar.google.com/citations?user=z84rLjoAAAAJ&hl=en&authuser=1) (ğŸ‡¦ğŸ‡ª Inception Institute of Artificial Intelligence)
    - ğŸ’¥ A gloden baseline for image-level polyp segmentation
    - It's racked up over 1,100 citations on [Google Scholar](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=kakwJ5QAAAAJ&authuser=1&citation_for_view=kakwJ5QAAAAJ:_B80troHkn4C), and counting!
    - Our [GitHub repo](https://github.com/DengPingFan/PraNet) is very popular with more than 400 stars!

<!-- One more thing, we ready some handbooks for your convenienice to dive into this research domain

- Must read papers related to colonoscopy [awesome-intelligent-colonoscopy](https://github.com/ai4colonoscopy/awesome-intelligent-colonoscopy) 
- [colonoscopy-dataset] -->

## ğŸ”­ Move Forward Bravely

This is just the start of building our Roman Empire ğŸ”±. Weâ€™re on a mission to make colonoscopies smarter, more accurate, and ultimately, save more lives. Want to join us on this exciting journey? Stay tuned, and letâ€™s revolutionize cancer prevention together! Feel free to reach out (ğŸ“§ gepengai.ji@gmail.com) if you're interested in collaborating and pushing the boundaries of intelligent colonoscopy.

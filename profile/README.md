![Untitled 001](https://github.com/user-attachments/assets/4ac24d1b-8725-473e-95e1-3ddcd08533a3)

## âœ¨ Enter AI4Colonoscopy -- where cutting-edge AI meets life-saving clinical practice

Hey there ğŸ‘‹ Weâ€™re not just making colonoscopies "more effective"; we're redefining what early detection can look like, and pushing the next frontier of intelligent healthcare.

With real-time lesion recognition and decision-support powered by deep learning, AI4Colonoscopy has the potential to significantly improve diagnostic accuracy, reduce missed lesions, and ultimately save lives.

Buckle up â€” weâ€™re diving into a future where intelligent colonoscopy becomes the new gold standard for cancer prevention.

## ğŸ™‹ News

- [2025/Dec/09] ğŸ”¥ğŸ”¥ Released the [Colon-X](https://github.com/ai4colonoscopy/Colon-X) project, focusing on a critical yet underexplored transition â€” evolving from multimodal understanding to clinical reasoning. Read our paper: https://arxiv.org/abs/2512.03667.
- [2024/Oct/30] ğŸ”¥ Released the [IntelliScope](https://github.com/ai4colonoscopy/IntelliScope) project, pushing colonoscopy research from pure visual analysis to multimodal analysis. Read our paper: https://arxiv.org/abs/2410.17241.
- [2024/Sep/01] Created the welcome page.

## ğŸ¥ Why Should You Care About Colonoscopy?

<p align="center">
  <img src="https://github.com/user-attachments/assets/0fc5d354-e4ab-4fcf-ad7d-a047f53075e7" width="1000px" />
  <br>
  <em> Illustration of a colonoscope inside large intestine (colon). (Image credit: https://www.vecteezy.com) </em>
</p>


Let's face it - colorectal cancer is a big deal. It's one of the top cancer killers worldwide. More details refer to latest research and news by Nature: ["subject - colorectal-cancer"](https://www.nature.com/subjects/colorectal-cancer) and ["subject - colonoscopy"](https://www.nature.com/subjects/colonoscopy). Here's the good news: we can often prevent it if we catch it early. That's where colonoscopies come in, as the best preventive measure. They're our best shot at finding and removing those sneaky precancerous polyps before they cause trouble to your body.

## ğŸ¤– Enter AI: The Game-Changer

<img align="right" src="https://github.com/user-attachments/assets/0de36f70-703b-49f4-ad6b-6c53000dc632" width="475px" />


We're using cutting-edge artificial intelligence to endower colonoscopy a major upgrade. Think of it as giving doctors a pair of super-AI-powered glasses that help them spot things they might otherwise miss.

That's why we're going to explore the critical role of AI in colonoscopy. Here's what AI brings to the table:

- ğŸ” **Improved polyp detection rates**
    - AI is like a tireless assistant, constantly scanning for even the tiniest polyps that human eyes might overlook.
- ğŸ¯ **High sensitivity in distinguishing precancerous polyps**
    - Not all polyps are created equal. AI can be trained to differentiate between the harmless ones and those that could become cancerous, helping doctors prioritize treatment.
- ğŸ–¼ï¸ **Enhanced overall precision of colon evaluation**
    - It's not just about spotting polyps. AI provides a comprehensive view of the colon, helping doctors make more accurate assessments.
- ğŸ˜€ **No added risk to colonoscopy**
    - Here's the best part - all these benefits come with zero additional risk to the patient. It's like getting a free upgrade on your health check!


## ğŸŒ Welcome to the World of Intelligent Colonoscopy!

Next are some of the research breakthroughs from our team that shaped the field of intelligent colonoscopy:

### ğŸ”´ Survey on Intelligent Colonoscopy Techniques

We introduce "ColonSurvey" project, contributing various useful resources for the community. We investigate 63 colonoscopy datasets and 137 deep learning models focused on colonoscopic scene perception, all sourced from leading conferences or journals since 2015. The below figure is a quick overview of our investigation; for a more detailed discussion, please refer to our paper in PDF format.

<p align="center">
    <img src="https://github.com/ai4colonoscopy/IntelliScope/raw/main/assets/colonsurvey.png"/> <br />
    <em> 
    Our investigation of 63 colonoscopy datasets and 137 deep learning models in colonoscopy.
    </em>
</p>


To better understand developments in this rapidly changing field and accelerate researchersâ€™ progress, we are building a [ğŸ“–paper reading list](https://docs.google.com/spreadsheets/d/1V_s99Jv9syzM6FPQAJVQqOFm5aqclmrYzNElY6BI18I/edit?usp=sharing), which includes a number of AI-based scientific studies on colonoscopy imaging from the past 12 years. 
*[UPDATE ON OCT-14-2024]* In detail, our online list contains:


- Colonoscopy datasets ğŸ”— [Google sheet](https://docs.google.com/spreadsheets/d/1V_s99Jv9syzM6FPQAJVQqOFm5aqclmrYzNElY6BI18I/edit?gid=358592785#gid=358592785)
- Colonoscopy models
   - Classification tasks ğŸ”— [Google sheet](https://docs.google.com/spreadsheets/d/1V_s99Jv9syzM6FPQAJVQqOFm5aqclmrYzNElY6BI18I/edit?gid=0#gid=0)
   - Detection tasks ğŸ”— [Google sheet](https://docs.google.com/spreadsheets/d/1V_s99Jv9syzM6FPQAJVQqOFm5aqclmrYzNElY6BI18I/edit?gid=1958057905#gid=1958057905)
   - Segmentation tasks ğŸ”— [Google sheet](https://docs.google.com/spreadsheets/d/1V_s99Jv9syzM6FPQAJVQqOFm5aqclmrYzNElY6BI18I/edit?gid=190887566#gid=190887566)
   - Vision language tasks ğŸ”— [Google sheet](https://docs.google.com/spreadsheets/d/1V_s99Jv9syzM6FPQAJVQqOFm5aqclmrYzNElY6BI18I/edit?gid=404456121#gid=404456121)
   - 3D analysis tasks (*supplementary content) ğŸ”— [Google sheet](https://docs.google.com/spreadsheets/d/1V_s99Jv9syzM6FPQAJVQqOFm5aqclmrYzNElY6BI18I/edit?gid=1052886329#gid=1052886329)

> [!note]
> ğŸ“Œ **Make our community great again.** If we miss your valuable work in google sheet, please add it and this project would be a nice platform to promote your work. Or anyone can inform us via email (ğŸ“® gepengai.ji@gmail.com) or push a PR in github. We will work on your request as soon as possible. Thank you for your active feedback.

### ğŸ”´ Highlight-A -- Image Analysis in Colonoscopy ğŸ”´

#### ğŸ¯ A.1. PraNet -- A Golden Baseline for Image Polyp Segmentation

[![arXiv](https://img.shields.io/badge/arXiv-2006.11392-b31b1b.svg)](https://arxiv.org/abs/2006.11392) [![GitHub Repo stars](https://img.shields.io/github/stars/DengPingFan/PraNet?style=flat&logo=github)](https://github.com/DengPingFan/PraNet)

- ğŸ“š **[Title]** PraNet: Parallel Reverse Attention Network for Polyp Segmentation ([Paper link](https://arxiv.org/abs/2006.11392) & [Code link](https://github.com/DengPingFan/PraNet))
- ğŸ† **[Info]** Accepted by [MICCAI 2020 (Oral Presentation)](https://link.springer.com/chapter/10.1007/978-3-030-59725-2_26) and has been cited over 2,100+ times (according to [Google Scholar](https://scholar.google.com/scholar?cites=15606470069856387091&as_sdt=2005&sciodt=0,5&hl=en) as of Dec 2024). Received [Most Influential Application Paper Award at the Jittor Developer Conference 2021](https://dengpingfan.github.io/papers/PraNet-Award.pdf) and [MICCAI Young Scientist Publication Impact Award 2025](https://miccai.org/index.php/about-miccai/awards/young-scientist-impact-award/).
- ğŸ›ï¸ **[Authors]** [Deng-Ping Fan](https://scholar.google.com/citations?user=kakwJ5QAAAAJ&hl=en) (ğŸ‡¦ğŸ‡ª Inception Institute of Artificial Intelligence), [Ge-Peng Ji](https://scholar.google.com/citations?user=oaxKYKUAAAAJ&hl=en&authuser=1) (ğŸ‡¨ğŸ‡³ Wuhan University), [Tao Zhou](https://scholar.google.com/citations?user=LPPsgWUAAAAJ&hl=en) (ğŸ‡¦ğŸ‡ª Inception Institute of Artificial Intelligence), [Geng Chen](https://scholar.google.com/citations?user=sJGCnjsAAAAJ&hl=en) (ğŸ‡¦ğŸ‡ª Inception Institute of Artificial Intelligence), [Huazhu Fu](https://scholar.google.com/citations?user=jCvUBYMAAAAJ&hl=en&authuser=1) (ğŸ‡¦ğŸ‡ª Inception Institute of Artificial Intelligence), [Jianbing Shen](https://scholar.google.com/citations?user=_Q3NTToAAAAJ&hl=en) (ğŸ‡¦ğŸ‡ª Inception Institute of Artificial Intelligence), [Ling Shao](https://scholar.google.com/citations?user=z84rLjoAAAAJ&hl=en&authuser=1) (ğŸ‡¦ğŸ‡ª Inception Institute of Artificial Intelligence)
- ğŸŒŸ **[Research Highlights]**
  - The **most influential and widely-used baseline** for image-level polyp segmentation, shaping subsequent research directions in the field.
  - Inspired by human visual behavior when examining lesions and their surroundings, we introduce the **Reverse Attention (RA) mechanism**, enabling the model to refine its focus on ambiguous regions.
  - Deliver state-of-the-art segmentation performance across five challenging polyp datasets. PraNet also achieved **1st Place** in the MediaEval 2020 colonoscopy polyp segmentation challenge.

#### ğŸ¯ A.2. PraNet v2 -- Extending Reverse Attention for Multi-class Medical Segmentation


### ğŸ”´ Highlight-B -- Video Analysis in Colonoscopy

#### ğŸ¯ B.1. PNS-Net -- A Super-efficient Model for Video Polyp Segmentation

[![arXiv](https://img.shields.io/badge/arXiv-2105.08468-b31b1b.svg)](https://arxiv.org/abs/2105.08468) [![GitHub Repo stars](https://img.shields.io/github/stars/GewelsJI/PNS-Net?style=flat&logo=github)](https://github.com/GewelsJI/PNS-Net)

- ğŸ“š **[Title]** Progressively Normalized Self-Attention Network for Video Polyp Segmentation ([Paper link](https://arxiv.org/abs/2105.08468) & [Code link](https://github.com/GewelsJI/PNS-Net))
- ğŸ† **[Info]** Accepted by [MICCAI 2021](https://link.springer.com/chapter/10.1007/978-3-030-87193-2_14) and received [MICCAI Student Travel Award](https://www.miccai2021.org/en/MICCAI-2021-TRAVEL-AWARDS.html)
- ğŸ›ï¸ **[Authors]** [Ge-Peng Ji](https://scholar.google.com/citations?user=oaxKYKUAAAAJ&hl=en&authuser=1) (ğŸ‡¦ğŸ‡ª Inception Institute of Artificial Intelligence), [Yu-Cheng Chou](https://scholar.google.com.tw/citations?user=YVNRBTcAAAAJ&hl) (ğŸ‡¨ğŸ‡³ Wuhan University), [Deng-Ping Fan](https://scholar.google.com/citations?user=kakwJ5QAAAAJ&hl=en) (ğŸ‡¦ğŸ‡ª Inception Institute of Artificial Intelligence), [Geng Chen](https://scholar.google.com/citations?user=sJGCnjsAAAAJ&hl=en&authuser=1) (ğŸ‡¦ğŸ‡ª Inception Institute of Artificial Intelligence), [Huazhu Fu](https://scholar.google.com/citations?user=jCvUBYMAAAAJ&hl=en&authuser=1) (ğŸ‡¦ğŸ‡ª Inception Institute of Artificial Intelligence), [Debesh Jha](https://scholar.google.com/citations?user=mMTyE68AAAAJ&hl=en) (ğŸ‡³ğŸ‡´ SimulaMet), [Ling Shao](https://scholar.google.com/citations?user=z84rLjoAAAAJ&hl=en&authuser=1) (ğŸ‡¦ğŸ‡ª Inception Institute of Artificial Intelligence)
- ğŸŒŸ **[Research Highlights]**
    - Propose **progressively normalized self-attention (PNS) module** to capture short-and long-term dependencies across colonoscopy frames.
    - Develop PNS-Net, a super-efficient VPS model, that **runs at ~140fps on a single RTX 2080Ti GPU**, making it highly practical for real-world endoscopy systems

#### ğŸ¯ B.2. SUN-SEG -- A Large-scale Benchmark for Video Polyp Segmentation

[![arXiv](https://img.shields.io/badge/arXiv-2203.14291-b31b1b.svg)](https://arxiv.org/abs/2203.14291) [![GitHub Repo stars](https://img.shields.io/github/stars/GewelsJI/VPS?style=flat&logo=github)](https://github.com/GewelsJI/VPS)

- ğŸ“š **[Title]** Video Polyp Segmentation: A Deep Learning Perspective ([Paper link](https://arxiv.org/abs/2203.14291) & [Code link](https://github.com/GewelsJI/VPS))
- ğŸ† **[Info]** Published in [Machine Intelligence Research 2022](https://link.springer.com/article/10.1007/s11633-022-1371-y)
- ğŸ›ï¸ **[Authors]** [Ge-Peng Ji](https://scholar.google.com/citations?user=oaxKYKUAAAAJ&hl=en&authuser=1) (ğŸ‡¦ğŸ‡º Australian National University), [Guobao Xiao](https://jsj.mju.edu.cn/2018/1014/c2248a67381/page.htm) (ğŸ‡¨ğŸ‡³ Minjiang University), [Yu-Cheng Chou](https://scholar.google.com.tw/citations?user=YVNRBTcAAAAJ&hl) (ğŸ‡ºğŸ‡¸ Johns Hopkins University), [Deng-Ping Fan](https://scholar.google.com/citations?user=kakwJ5QAAAAJ&hl=en) (ğŸ‡¨ğŸ‡­ ETH ZÃ¼rich), [Kai Zhao](https://scholar.google.com/citations?hl=en&user=zR5JbZUAAAAJ) (ğŸ‡ºğŸ‡¸ University of California, Los Angeles), [Geng Chen](https://scholar.google.com/citations?user=sJGCnjsAAAAJ&hl=en&authuser=1) (ğŸ‡¦ğŸ‡ª Inception Institute of Artificial Intelligence), [Luc Van Gool](https://scholar.google.com/citations?user=TwMib_QAAAAJ&hl=en&authuser=1) (ğŸ‡¨ğŸ‡­ ETH ZÃ¼rich)
- ğŸŒŸ **[Research Highlights]**
  - We construct the **largest-scale and high-quality per-frame annotated VPS dataset**, SUN-SEG, which includes 158,690 video frames along with diverse types of expert labels, including object mask, boundary, scribble, polygon, and visual attribute.
  - Paving the way for future research in colonoscopy video analysis.


### ğŸ”´ Highlight-C -- Multimodal Analysis in Colonoscopy

#### ğŸ¯ C.1. ColonINST & ColonGPT -- Pioneering Multimodal Intelligence in Colonoscopy

[![arXiv](https://img.shields.io/badge/arXiv-2410.17241-b31b1b.svg)](https://arxiv.org/abs/2410.17241) [![GitHub Repo stars](https://img.shields.io/github/stars/ai4colonoscopy/IntelliScope?style=flat&logo=github)](https://github.com/ai4colonoscopy/IntelliScope)

- ğŸ“š **[Title]** IntelliScope: Pioneering Multimodal Intelligence in Colonoscopy ([Paper link](https://arxiv.org/abs/2410.17241) & [Code link](https://github.com/ai4colonoscopy/IntelliScope))
- ğŸ† **[Info]** Accepted by Machine Intelligence Research 2026
- ğŸ›ï¸ **[Authors]** [Ge-Peng Ji](https://scholar.google.com/citations?hl=en&authuser=1&user=oaxKYKUAAAAJ) (ğŸ‡¦ğŸ‡º Australian National University), Jingyi Liu (ğŸ‡¯ğŸ‡µ Keio University), [Peng Xu](https://scholar.google.com/citations?user=9_v4tC0AAAAJ&hl=en) (ğŸ‡¨ğŸ‡³ Tsinghua University), [Nick Barnes](https://scholar.google.com/citations?hl=en&user=yMXs1WcAAAAJ) (ğŸ‡¦ğŸ‡º Australian National University), [Fahad Shahbaz Khan](https://scholar.google.com/citations?user=zvaeYnUAAAAJ&hl=en&authuser=1) (ğŸ‡¦ğŸ‡ª MBZUAI), [Salman Khan](https://scholar.google.com/citations?user=M59O9lkAAAAJ&hl=en&authuser=1) (ğŸ‡¦ğŸ‡ª MBZUAI), [Deng-Ping Fan*](https://scholar.google.com/citations?user=kakwJ5QAAAAJ&hl=en) (ğŸ‡¨ğŸ‡³ Nankai University)
- ğŸŒŸ **[Research Highlights]** This year, weâ€™re taking intelligent colonoscopy to the next level, a multimodal world, with three groundbreaking initiatives:
    - ğŸ’¥ Collecting a large-scale multimodal instruction tuning dataset ColonINST, featuring 300K+ colonoscopy images, 62 categories, 128K+ GPT-4V-generated medical captions, and 450K+ human-machine dialogues.
    - ğŸ’¥ Developing the first multimodal language model ColonGPT that can handle conversational tasks based on user preferences.
    - ğŸ’¥ Launching a multimodal benchmark to enable fair and rapid comparisons going forward.

#### ğŸ¯ C.2. Colon-X -- Advancing colonoscopy from Multimodal Understanding to Clinical Reasoning

[![arXiv](https://img.shields.io/badge/arXiv-2512.03667-b31b1b.svg)](https://arxiv.org/abs/2512.03667) [![GitHub Repo stars](https://img.shields.io/github/stars/ai4colonoscopy/Colon-X?style=flat&logo=github)](https://github.com/ai4colonoscopy/Colon-X)

- ğŸ“š **[Title]** Colon-X: Advancing Intelligent Colonoscopy from Multimodal Understanding to Clinical Reasoning ([arXiv Paper](https://arxiv.org/abs/2512.03667) & [Project page](https://github.com/ai4colonoscopy/Colon-X))
- ğŸ›ï¸ **[Authors]** [Ge-Peng Ji](https://scholar.google.com/citations?hl=en&authuser=1&user=oaxKYKUAAAAJ) (ğŸ‡¦ğŸ‡º Australian National University), Jingyi Liu (ğŸ‡¨ğŸ‡³ Nankai University), [Deng-Ping Fan*](https://scholar.google.com/citations?user=kakwJ5QAAAAJ&hl=en) (ğŸ‡¨ğŸ‡³ Nankai University), [Nick Barnes](https://scholar.google.com/citations?hl=en&user=yMXs1WcAAAAJ) (ğŸ‡¦ğŸ‡º Australian National University)
- ğŸŒŸ **[Research Highlights]** In this project, we are pushing the boundaries of intelligent colonoscopy by transitioning from multimodal understanding to clinical reasoning. Our key contributions include:
    - ?
    - ?
    - ?

## ğŸ’¬ Discussion Forum

This is just the start of building our Roman Empire ğŸ”±. Weâ€™re on a mission to make colonoscopies smarter, more accurate, and ultimately, save more lives. Want to join us on this exciting journey? Welcome to our [AI4Colonoscopy Discussion Forum](https://github.com/orgs/ai4colonoscopy/discussions/1)

  - [(SUBFORUM#1) ask any questions](https://github.com/orgs/ai4colonoscopy/discussions/categories/any-q-a) ğŸ˜¥ *â€œè®ºæ–‡ä¸­é‡è§äº†é—®é¢˜ï¼Ÿä»£ç ä¸ä¼šè·‘ï¼Ÿâ€œ*
  - [(SUBFORUM#2) showcase/promote your work](https://github.com/orgs/ai4colonoscopy/discussions/categories/show-tell) ğŸ˜¥ *â€æƒ³å¢åŠ è®ºæ–‡å½±å“åŠ›ï¼Ÿå¦‚ä½•å‘ç¤¾åŒºå®£ä¼ è‡ªå·±çš„å·¥ä½œï¼Ÿâ€œ*
  - [(SUBFORUM#3) access data resources](https://github.com/orgs/ai4colonoscopy/discussions/categories/data-helpdesk) ğŸ˜¥ *â€œä¸‹è½½ä¸åˆ°æ•°æ®ï¼Ÿå¦‚ä½•ä½¿ç”¨/å¤„ç†æ‰‹å¤´çš„æ•°æ®ï¼Ÿâ€*
  - [(SUBFORUM#4) share research ideas](https://github.com/orgs/ai4colonoscopy/discussions/categories/ideas-collaborations) ğŸ˜¥ *â€æ‰¾ä¸åˆ°åˆä½œè€…ï¼Ÿæƒ³ä¸å‡ºæœ‰è¶£çš„ideaï¼Ÿâ€œ*

## ğŸ§© Collaborating towards the neXt frontier

We are actively looking for potential collaborators to help push this community forward â€” especially **hospitals or medical institutions that can provide diverse, real-world clinical colonoscopy data (eg., data across different devices, modalities, patient populations, and clinical workflows)**. If youâ€™re interested in contributing or partnering with us, weâ€™d be very happy to connect.

Weâ€™re still on the journey toward building truly intelligent colonoscopy systems, and this project is very much under active development. We warmly welcome any feedback, ideas, or suggestions that can help shape its future.

For any inquiries or thoughts youâ€™d like to share, feel free to reach out to us at ğŸ“§ gepengai.ji@gmail.com
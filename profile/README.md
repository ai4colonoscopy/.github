![Untitled 001](https://github.com/user-attachments/assets/4ac24d1b-8725-473e-95e1-3ddcd08533a3)

## âœ¨ Enter AI4Colonoscopy -- where cutting-edge AI meets life-saving clinical practice

Hey there ğŸ‘‹ Weâ€™re not just making colonoscopies "more effective"; we're redefining what early detection can look like, and pushing the next frontier of intelligent healthcare.

With real-time lesion recognition and decision-support powered by deep learning, AI4Colonoscopy has the potential to significantly improve diagnostic accuracy, reduce missed lesions, and ultimately save lives.

Buckle up â€” weâ€™re diving into a future where intelligent colonoscopy becomes the new gold standard for cancer prevention.

## ğŸ™‹ News

- [2025/Dec/09] ğŸ”¥ğŸ”¥ Released the [Colon-X](https://github.com/ai4colonoscopy/Colon-X) project, focusing on a critical yet underexplored transition â€” evolving from multimodal understanding to clinical reasoning. Read our paper: https://arxiv.org/abs/2512.03667.
- [2024/Oct/30] ğŸ”¥ Released the [IntelliScope](https://github.com/ai4colonoscopy/IntelliScope) project, pushing colonoscopy research from pure visual analysis to multimodal analysis. Read our paper: https://arxiv.org/abs/2410.17241.
- [2024/Sep/01] Created the welcome page.

## ğŸ¥ Why Should You Care About Colonoscopy?

<p align="center">
  <img src="https://github.com/user-attachments/assets/0fc5d354-e4ab-4fcf-ad7d-a047f53075e7" width="1000px" />
  <br>
  <em> Illustration of a colonoscope inside large intestine (colon). (Image credit: https://www.vecteezy.com) </em>
</p>


Let's face it - colorectal cancer is a big deal. It's one of the top cancer killers worldwide. More details refer to latest research and news by Nature: ["subject - colorectal-cancer"](https://www.nature.com/subjects/colorectal-cancer) and ["subject - colonoscopy"](https://www.nature.com/subjects/colonoscopy). Here's the good news: we can often prevent it if we catch it early. That's where colonoscopies come in, as the best preventive measure. They're our best shot at finding and removing those sneaky precancerous polyps before they cause trouble to your body.

## ğŸ¤– Enter AI: The Game-Changer

<img align="right" src="https://github.com/user-attachments/assets/0de36f70-703b-49f4-ad6b-6c53000dc632" width="475px" />


We're using cutting-edge artificial intelligence to endower colonoscopy a major upgrade. Think of it as giving doctors a pair of super-AI-powered glasses that help them spot things they might otherwise miss.

That's why we're going to explore the critical role of AI in colonoscopy. Here's what AI brings to the table:

- ğŸ” **Improved polyp detection rates**
    - AI is like a tireless assistant, constantly scanning for even the tiniest polyps that human eyes might overlook.
- ğŸ¯ **High sensitivity in distinguishing precancerous polyps**
    - Not all polyps are created equal. AI can be trained to differentiate between the harmless ones and those that could become cancerous, helping doctors prioritize treatment.
- ğŸ–¼ï¸ **Enhanced overall precision of colon evaluation**
    - It's not just about spotting polyps. AI provides a comprehensive view of the colon, helping doctors make more accurate assessments.
- ğŸ˜€ **No added risk to colonoscopy**
    - Here's the best part - all these benefits come with zero additional risk to the patient. It's like getting a free upgrade on your health check!


## ğŸŒ Welcome to the World of Intelligent Colonoscopy!

Next are some of the research breakthroughs from our team that shaped the field of intelligent colonoscopy:

### ğŸ”´ Survey on Intelligent Colonoscopy Techniques

> [!note]
> ğŸ“Œ **Make our community great again.** If we miss your valuable work in google sheet, please add it and this project would be a nice platform to promote your work. Or anyone can inform us via email ([ğŸ“®gepengai.ji@gmail.com](mailto:gepengai.ji@gmail.com)) or push a PR in github. We will work on your request as soon as possible. Thank you for your active feedback.


We introduce "ColonSurvey" via investigating 63 colonoscopy datasets and 137 deep learning models focused on colonoscopic scene perception, all sourced from leading conferences or journals since 2015. The below figure is a quick overview of our investigation; for a more detailed discussion, please refer to our survey paper: [Frontiers in Intelligent Colonoscopy](https://arxiv.org/abs/2410.17241).

<p align="center">
    <img src="https://github.com/ai4colonoscopy/IntelliScope/raw/main/assets/colonsurvey.png"/> <br />
    <em> 
    Our investigation of 63 colonoscopy datasets and 137 deep learning models in colonoscopy.
    </em>
</p>

To better understand developments in this rapidly changing field and accelerate researchersâ€™ progress, we are building a [ğŸ“–paper reading list](https://docs.google.com/spreadsheets/d/1V_s99Jv9syzM6FPQAJVQqOFm5aqclmrYzNElY6BI18I/edit?usp=sharing), which includes a number of AI-based scientific studies on colonoscopy imaging from the past 12 years. 

*[UPDATE ON OCT-14-2024]* In detail, our online list contains:

- Colonoscopy datasets ğŸ”— [Google sheet](https://docs.google.com/spreadsheets/d/1V_s99Jv9syzM6FPQAJVQqOFm5aqclmrYzNElY6BI18I/edit?gid=358592785#gid=358592785)
- Colonoscopy models
   - Classification tasks ğŸ”— [Google sheet](https://docs.google.com/spreadsheets/d/1V_s99Jv9syzM6FPQAJVQqOFm5aqclmrYzNElY6BI18I/edit?gid=0#gid=0)
   - Detection tasks ğŸ”— [Google sheet](https://docs.google.com/spreadsheets/d/1V_s99Jv9syzM6FPQAJVQqOFm5aqclmrYzNElY6BI18I/edit?gid=1958057905#gid=1958057905)
   - Segmentation tasks ğŸ”— [Google sheet](https://docs.google.com/spreadsheets/d/1V_s99Jv9syzM6FPQAJVQqOFm5aqclmrYzNElY6BI18I/edit?gid=190887566#gid=190887566)
   - Vision language tasks ğŸ”— [Google sheet](https://docs.google.com/spreadsheets/d/1V_s99Jv9syzM6FPQAJVQqOFm5aqclmrYzNElY6BI18I/edit?gid=404456121#gid=404456121)
   - 3D analysis tasks (*supplementary content) ğŸ”— [Google sheet](https://docs.google.com/spreadsheets/d/1V_s99Jv9syzM6FPQAJVQqOFm5aqclmrYzNElY6BI18I/edit?gid=1052886329#gid=1052886329)


### ğŸ”´ Highlight-A -- Image Analysis in Colonoscopy

#### ğŸ¯ A.1. PraNet -- Bring Reverse Attention for Segmenting Ambiguous Camouflaged Lesions

[![arXiv](https://img.shields.io/badge/arXiv-2006.11392-b31b1b.svg)](https://arxiv.org/abs/2006.11392) [![GitHub Repo stars](https://img.shields.io/github/stars/DengPingFan/PraNet?style=flat&logo=github)](https://github.com/DengPingFan/PraNet)

<p align="center">
    <img src="https://github.com/user-attachments/assets/9dfa8007-af53-4213-9bec-e548fe159261"/> <br />
    <em> 
    Photo for MICCAI Young Scientist Publication Impact Award 2025 
    </em>
</p>

> [!NOTE]
> ğŸ“Œ **Our Motivation of Reverse Attention -- "Attention Reorienting Mechanism"**
>
> The attention reorienting refers to the phenomenon where human visual attention does not remain fixed, but instead **shifts periodically and rhythmically across different regions of the visual field**.
> 
> This process is especially prominent **when the brain encounters ambiguous, low-contrast, or uncertain information**. Rather than committing to a single viewpoint, the visual system continuously revisits these regions to refine perception, reduce uncertainty, and improve recognition accuracy.
>
> A classic study in visual neuroscience, "[ORIENTING OF ATTENTION. Posner, 1980](https://www2.psychology.uiowa.edu/faculty/hollingworth/prosem/scan_posner_1980.pdf)" indicate several key aspects of this phenomenon:
> - Attention shifts even without eye movements (covert reorientation).
> - These shifts tend to occur periodically -- the brain scans â†’ evaluates â†’ rescans.
> - Ambiguous or important regions receive more frequent revisits.
> 
> In short, this mechanism is crucial for tasks such as detecting lesions, identifying boundaries, or resolving camouflaged objects. **Our Reverse Attention (RA) mimics this periodic reorientation** by refocusing on uncertain areas after processing more obvious regions.

- ğŸ“š **[Title]** PraNet: Parallel Reverse Attention Network for Polyp Segmentation ([Paper link](https://arxiv.org/abs/2006.11392) & [Code link](https://github.com/DengPingFan/PraNet))
- ğŸ† **[Info]** Accepted by [MICCAI 2020 (Oral Presentation)](https://link.springer.com/chapter/10.1007/978-3-030-59725-2_26) and has been cited over 2,100+ times (according to [Google Scholar](https://scholar.google.com/scholar?cites=15606470069856387091&as_sdt=2005&sciodt=0,5&hl=en) as of Dec 2024). Received [Most Influential Application Paper Award at the Jittor Developer Conference 2021](https://dengpingfan.github.io/papers/PraNet-Award.pdf) and [MICCAI Young Scientist Publication Impact Award 2025](https://miccai.org/index.php/about-miccai/awards/young-scientist-impact-award/).
- ğŸ›ï¸ **[Authors]** [Deng-Ping Fan](https://scholar.google.com/citations?user=kakwJ5QAAAAJ&hl=en) (ğŸ‡¦ğŸ‡ª Inception Institute of Artificial Intelligence), [Ge-Peng Ji](https://scholar.google.com/citations?user=oaxKYKUAAAAJ&hl=en&authuser=1) (ğŸ‡¨ğŸ‡³ Wuhan University), [Tao Zhou](https://scholar.google.com/citations?user=LPPsgWUAAAAJ&hl=en) (ğŸ‡¦ğŸ‡ª Inception Institute of Artificial Intelligence), [Geng Chen](https://scholar.google.com/citations?user=sJGCnjsAAAAJ&hl=en) (ğŸ‡¦ğŸ‡ª Inception Institute of Artificial Intelligence), [Huazhu Fu](https://scholar.google.com/citations?user=jCvUBYMAAAAJ&hl=en&authuser=1) (ğŸ‡¦ğŸ‡ª Inception Institute of Artificial Intelligence), [Jianbing Shen](https://scholar.google.com/citations?user=_Q3NTToAAAAJ&hl=en) (ğŸ‡¦ğŸ‡ª Inception Institute of Artificial Intelligence), [Ling Shao](https://scholar.google.com/citations?user=z84rLjoAAAAJ&hl=en&authuser=1) (ğŸ‡¦ğŸ‡ª Inception Institute of Artificial Intelligence)
- ğŸŒŸ **[Research Highlights]**
  - The **most influential and widely-used baseline** for image-level polyp segmentation, shaping subsequent research directions in the field.
  - Inspired by human visual behavior when examining lesions and their surroundings, we introduce the **Reverse Attention (RA) mechanism**, enabling the model to refine its focus on ambiguous regions.
  - Deliver state-of-the-art segmentation performance across five challenging polyp datasets. PraNet also achieved **1st Place** in the MediaEval 2020 colonoscopy polyp segmentation challenge.
- ğŸ“ˆ **[Citation]**
    ```bibliography
    @inproceedings{fan2020pranet,
      title={Pranet: Parallel reverse attention network for polyp segmentation},
      author={Fan, Deng-Ping and Ji, Ge-Peng and Zhou, Tao and Chen, Geng and Fu, Huazhu and Shen, Jianbing and Shao, Ling},
      booktitle={International conference on medical image computing and computer-assisted intervention},
      pages={263--273},
      year={2020},
      organization={Springer}
    }
    ```


#### ğŸ¯ A.2. PraNet v2 -- Adapting Reverse Attention for Multi-class Medical Segmentation

[![arXiv](https://img.shields.io/badge/arXiv-2504.10986-b31b1b.svg)](https://arxiv.org/abs/2504.10986) [![GitHub Repo stars](https://img.shields.io/github/stars/ai4colonoscopy/PraNet-V2?style=flat&logo=github)](https://github.com/ai4colonoscopy/PraNet-V2)

<div align='left'>
<video src="https://github.com/user-attachments/assets/6f4e91dd-f13b-4597-a896-26031656e5e6" controls width="600" />
</div>

- ğŸ“š **[Title]** PraNet-V2: Dual-Supervised Reverse Attention for Medical Image Segmentation ([Paper link](https://arxiv.org/abs/2504.10986) & [Code link](https://github.com/ai4colonoscopy/PraNet-V2))
- ğŸ† **[Info]** Accepted by Computational Visual Media 2025
- ğŸ›ï¸ **[Authors]** [Bo-Cheng Hu](https://scholar.google.com/citations?user=VBb03aoAAAAJ) (ğŸ‡¨ğŸ‡³ Nankai University), [Ge-Peng Ji](https://scholar.google.com/citations?hl=en&authuser=1&user=oaxKYKUAAAAJ) (ğŸ‡¦ğŸ‡º Australian National University), [Dian Shao](https://scholar.google.com/citations?user=amxDSLoAAAAJ) (ğŸ‡¨ğŸ‡³ Northwest Polytechnical University, [Deng-Ping Fan*](https://scholar.google.com/citations?user=kakwJ5QAAAAJ&hl=en) (ğŸ‡¨ğŸ‡³ Nankai University)
- ğŸŒŸ **[Research Highlights]**
  - We extend the Reverse Attention (RA) mechanism from binary polyp segmentation (ie., ours [PraNet-V1](https://arxiv.org/abs/2006.11392) published at MICCAI2020) to **multi-class** medical image segmentation.
  - We introduce a **dual-supervised RA learning strategy**, which incorporates both primary and auxiliary supervision to enhance feature representation.
  - PraNet-V2 achieves state-of-the-art performance on multiple challenging medical image segmentation datasets, demonstrating its versatility and effectiveness across various tasks.  
- ğŸ“ˆ **[Citation]**
    ```bibliography
    @article{hu2025pranet,
      title={PraNet-V2: Dual-Supervised Reverse Attention for Medical Image Segmentation},
      author={Hu, Bo-Cheng and Ji, Ge-Peng and Shao, Dian and Fan, Deng-Ping},
      journal={Computational Visual Media},
      year={2025}
    }
    ```

### ğŸ”´ Highlight-B -- Video Analysis in Colonoscopy

#### ğŸ¯ B.1. PNS-Net -- A Super-efficient Model for Video Polyp Segmentation

[![arXiv](https://img.shields.io/badge/arXiv-2105.08468-b31b1b.svg)](https://arxiv.org/abs/2105.08468) [![GitHub Repo stars](https://img.shields.io/github/stars/GewelsJI/PNS-Net?style=flat&logo=github)](https://github.com/GewelsJI/PNS-Net)

<p align="center">
    <img src="https://github.com/GewelsJI/PNS-Net/raw/main/imgs/VideoPresentation-min.gif"/> <br />
    <em> 
    Visualization --
     1st row: Input colonoscopy video frames; 
     2nd row: Ground-truth masks; 
     3rd row: Predicted masks by our PNS-Net.
    </em>
</p>

- ğŸ“š **[Title]** Progressively Normalized Self-Attention Network for Video Polyp Segmentation ([Paper link](https://arxiv.org/abs/2105.08468) & [Code link](https://github.com/GewelsJI/PNS-Net))
- ğŸ† **[Info]** Accepted by [MICCAI 2021](https://link.springer.com/chapter/10.1007/978-3-030-87193-2_14) and received [MICCAI Student Travel Award](https://www.miccai2021.org/en/MICCAI-2021-TRAVEL-AWARDS.html)
- ğŸ›ï¸ **[Authors]** [Ge-Peng Ji](https://scholar.google.com/citations?user=oaxKYKUAAAAJ&hl=en&authuser=1) (ğŸ‡¦ğŸ‡ª Inception Institute of Artificial Intelligence), [Yu-Cheng Chou](https://scholar.google.com.tw/citations?user=YVNRBTcAAAAJ&hl) (ğŸ‡¨ğŸ‡³ Wuhan University), [Deng-Ping Fan](https://scholar.google.com/citations?user=kakwJ5QAAAAJ&hl=en) (ğŸ‡¦ğŸ‡ª Inception Institute of Artificial Intelligence), [Geng Chen](https://scholar.google.com/citations?user=sJGCnjsAAAAJ&hl=en&authuser=1) (ğŸ‡¦ğŸ‡ª Inception Institute of Artificial Intelligence), [Huazhu Fu](https://scholar.google.com/citations?user=jCvUBYMAAAAJ&hl=en&authuser=1) (ğŸ‡¦ğŸ‡ª Inception Institute of Artificial Intelligence), [Debesh Jha](https://scholar.google.com/citations?user=mMTyE68AAAAJ&hl=en) (ğŸ‡³ğŸ‡´ SimulaMet), [Ling Shao](https://scholar.google.com/citations?user=z84rLjoAAAAJ&hl=en&authuser=1) (ğŸ‡¦ğŸ‡ª Inception Institute of Artificial Intelligence)
- ğŸŒŸ **[Research Highlights]**
    - Propose **progressively normalized self-attention (PNS) module** to capture short-and long-term dependencies across colonoscopy frames.
    - Develop PNS-Net, a super-efficient VPS model, that **runs at ~140fps on a single RTX 2080Ti GPU**, making it highly practical for real-world endoscopy systems
- ğŸ“ˆ **[Citation]**
    ```bibliography
    @inproceedings{ji2021progressively,
      title={Progressively normalized self-attention network for video polyp segmentation},
      author={Ji, Ge-Peng and Chou, Yu-Cheng and Fan, Deng-Ping and Chen, Geng and Fu, Huazhu and Jha, Debesh and Shao, Ling},
      booktitle={International conference on medical image computing and computer-assisted intervention},
      pages={142--152},
      year={2021},
      organization={Springer}
    }
    ```


#### ğŸ¯ B.2. SUN-SEG -- A Large-scale Benchmark for Video Polyp Segmentation

[![arXiv](https://img.shields.io/badge/arXiv-2203.14291-b31b1b.svg)](https://arxiv.org/abs/2203.14291) [![GitHub Repo stars](https://img.shields.io/github/stars/GewelsJI/VPS?style=flat&logo=github)](https://github.com/GewelsJI/VPS)

<p align="center">
    <img src="https://github.com/GewelsJI/VPS/raw/main/assets/background-min.gif"/> <br />
    <em> 
    Sample gallery from our SUN-SEG dataset.
    </em>
</p>

- ğŸ“š **[Title]** Video Polyp Segmentation: A Deep Learning Perspective ([Paper link](https://arxiv.org/abs/2203.14291) & [Code link](https://github.com/GewelsJI/VPS))
- ğŸ† **[Info]** Published in [Machine Intelligence Research 2022](https://link.springer.com/article/10.1007/s11633-022-1371-y)
- ğŸ›ï¸ **[Authors]** [Ge-Peng Ji](https://scholar.google.com/citations?user=oaxKYKUAAAAJ&hl=en&authuser=1) (ğŸ‡¦ğŸ‡º Australian National University), [Guobao Xiao](https://jsj.mju.edu.cn/2018/1014/c2248a67381/page.htm) (ğŸ‡¨ğŸ‡³ Minjiang University), [Yu-Cheng Chou](https://scholar.google.com.tw/citations?user=YVNRBTcAAAAJ&hl) (ğŸ‡ºğŸ‡¸ Johns Hopkins University), [Deng-Ping Fan](https://scholar.google.com/citations?user=kakwJ5QAAAAJ&hl=en) (ğŸ‡¨ğŸ‡­ ETH ZÃ¼rich), [Kai Zhao](https://scholar.google.com/citations?hl=en&user=zR5JbZUAAAAJ) (ğŸ‡ºğŸ‡¸ University of California, Los Angeles), [Geng Chen](https://scholar.google.com/citations?user=sJGCnjsAAAAJ&hl=en&authuser=1) (ğŸ‡¦ğŸ‡ª Inception Institute of Artificial Intelligence), [Luc Van Gool](https://scholar.google.com/citations?user=TwMib_QAAAAJ&hl=en&authuser=1) (ğŸ‡¨ğŸ‡­ ETH ZÃ¼rich)
- ğŸŒŸ **[Research Highlights]**
  - We construct the **largest-scale and high-quality per-frame annotated VPS dataset**, SUN-SEG, which includes 158,690 video frames along with diverse types of expert labels, including object mask, boundary, scribble, polygon, and visual attribute.
  - Paving the way for future research in colonoscopy video analysis.
- ğŸ“ˆ **[Citation]**
    ```bibliography
    @article{ji2022video,
      title={Video polyp segmentation: A deep learning perspective},
      author={Ji, Ge-Peng and Xiao, Guobao and Chou, Yu-Cheng and Fan, Deng-Ping and Zhao, Kai and Chen, Geng and Van Gool, Luc},
      journal={Machine Intelligence Research},
      volume={19},
      number={6},
      pages={531--549},
      year={2022},
      publisher={Springer}
    }
    ```

### ğŸ”´ Highlight-C -- Multimodal Analysis in Colonoscopy

#### ğŸ¯ C.1. ColonINST & ColonGPT -- Pioneering Multimodal Intelligence in Colonoscopy

[![arXiv](https://img.shields.io/badge/arXiv-2410.17241-b31b1b.svg)](https://arxiv.org/abs/2410.17241) [![GitHub Repo stars](https://img.shields.io/github/stars/ai4colonoscopy/IntelliScope?style=flat&logo=github)](https://github.com/ai4colonoscopy/IntelliScope)

<p align="center">
    <img src="https://github.com/ai4colonoscopy/IntelliScope/raw/main/assets/coloninst-overview.png"/> <br />
    <em> 
    <b>Details of our multimodal instruction tuning dataset, ColonINST.</b> (a) Three sequential steps to create the instruction tuning dataset for multimodal research. (b) Numbers of colonoscopy images designated for training, validation, and testing purposes. (c) Data taxonomy of three-level categories. (d) A word cloud of the category distribution by name size. (e) Caption generation pipeline using the VL prompting mode of GPT-4V. (f) Numbers of human-machine dialogues created for four downstream tasks.
    </em>
</p>


- ğŸ“š **[Title]** Frontiers in Intelligent Colonoscopy ([Paper link](https://arxiv.org/abs/2410.17241) & [Code link](https://github.com/ai4colonoscopy/IntelliScope))
- ğŸ† **[Info]** Accepted by Machine Intelligence Research 2026
- ğŸ›ï¸ **[Authors]** [Ge-Peng Ji](https://scholar.google.com/citations?hl=en&authuser=1&user=oaxKYKUAAAAJ) (ğŸ‡¦ğŸ‡º Australian National University), Jingyi Liu (ğŸ‡¯ğŸ‡µ Keio University), [Peng Xu](https://scholar.google.com/citations?user=9_v4tC0AAAAJ&hl=en) (ğŸ‡¨ğŸ‡³ Tsinghua University), [Nick Barnes](https://scholar.google.com/citations?hl=en&user=yMXs1WcAAAAJ) (ğŸ‡¦ğŸ‡º Australian National University), [Fahad Shahbaz Khan](https://scholar.google.com/citations?user=zvaeYnUAAAAJ&hl=en&authuser=1) (ğŸ‡¦ğŸ‡ª MBZUAI), [Salman Khan](https://scholar.google.com/citations?user=M59O9lkAAAAJ&hl=en&authuser=1) (ğŸ‡¦ğŸ‡ª MBZUAI), [Deng-Ping Fan*](https://scholar.google.com/citations?user=kakwJ5QAAAAJ&hl=en) (ğŸ‡¨ğŸ‡³ Nankai University)
- ğŸŒŸ **[Research Highlights]** This year, weâ€™re taking intelligent colonoscopy to the next level, a multimodal world, with three groundbreaking initiatives:
    - ğŸ’¥ Collecting a large-scale multimodal instruction tuning dataset **ColonINST**, featuring 300K+ colonoscopy images, 62 categories, 128K+ GPT-4V-generated medical captions, and 450K+ human-machine dialogues.
    - ğŸ’¥ Developing the first multimodal language model **ColonGPT** that can handle conversational tasks based on user preferences.
    - Launching a multimodal benchmark to enable fair and rapid comparisons going forward.
- ğŸ“ˆ **[Citation]**
    ```bibliography
    @article{ji2026frontiers,
      title={Frontiers in intelligent colonoscopy},
      author={Ji, Ge-Peng and Liu, Jingyi and Xu, Peng and Barnes, Nick and Khan, Fahad Shahbaz and Khan, Salman and Fan, Deng-Ping},
      journal={Machine Intelligence Research},
      year={2026}
    }
    ```


#### ğŸ¯ C.2. Colon-X -- Advancing colonoscopy from Multimodal Understanding to Clinical Reasoning

[![arXiv](https://img.shields.io/badge/arXiv-2512.03667-b31b1b.svg)](https://arxiv.org/abs/2512.03667) [![GitHub Repo stars](https://img.shields.io/github/stars/ai4colonoscopy/Colon-X?style=flat&logo=github)](https://github.com/ai4colonoscopy/Colon-X)

<p align="center">
    <img src="https://github.com/ai4colonoscopy/Colon-X/raw/main/assets/teaser_figure.png"/> <br />
    <em> 
    <b>Research roadmap of our Colon-X project.</b> Building upon the most comprehensive multimodal colonoscopy dataset (ColonVQA), we propel a pivotal transition in intelligent colonoscopy, evolving from multimodal understanding (ColonEval and ColonPert) to clinical reasoning (ColonReason and ColonR1). These efforts collectively illuminate the path to next-generation advances in clinical colonoscopy and broader medical applications.
    </em>
</p>


- ğŸ“š **[Title]** Colon-X: Advancing Intelligent Colonoscopy from Multimodal Understanding to Clinical Reasoning ([arXiv Paper](https://arxiv.org/abs/2512.03667) & [Project page](https://github.com/ai4colonoscopy/Colon-X))
- ğŸ† **[Info]** Under reviewing
- ğŸ›ï¸ **[Authors]** [Ge-Peng Ji](https://scholar.google.com/citations?hl=en&authuser=1&user=oaxKYKUAAAAJ) (ğŸ‡¦ğŸ‡º Australian National University), Jingyi Liu (ğŸ‡¨ğŸ‡³ Nankai University), [Deng-Ping Fan*](https://scholar.google.com/citations?user=kakwJ5QAAAAJ&hl=en) (ğŸ‡¨ğŸ‡³ Nankai University), [Nick Barnes](https://scholar.google.com/citations?hl=en&user=yMXs1WcAAAAJ) (ğŸ‡¦ğŸ‡º Australian National University)
- ğŸŒŸ **[Research Highlights]** In this project, we are pushing the boundaries of intelligent colonoscopy by transitioning from multimodal understanding to clinical reasoning. Our key contributions are three-fold:
    - We introduce ColonVQA, the **most extensive** (1.1+ million VQA entries), **category-rich** (212,742 images across 76 clinically meaningful findings), and **task-diverse** (18 multimodal tasks organized within a five-level taxonomy) dataset ever built for multimodal colonoscopy analysis. 
    - We characterize two multimodal understanding behaviors â€“ generalizability (**ColonEval**) and reliability (**ColonPert**) â€“ in colonoscopy tasks, and reveal that clinical outputs from leading MLLMs remain far from robust and trustworthy.
    - ğŸ’¥ We propose a reasoning-focused solution. It includes **ColonReason**, a reasoning dataset annotated by a multi-expert debating pipeline, and **ColonR1**, an R1-styled model enhanced with task-adaptive rewards and gradient-stable optimization, setting a new cutting-edge baseline for colonoscopy analysis.
- ğŸ“ˆ **[Citation]**
    ```bibliography
    @article{ji2025colon,
      title={Colon-X: Advancing Intelligent Colonoscopy from Multimodal Understanding to Clinical Reasoning},
      author={Ji, Ge-Peng and Liu, Jingyi and Fan, Deng-Ping and Barnes, Nick},
      journal={arXiv preprint arXiv:2512.03667},
      year={2025}
    }
    ```

## ğŸ§© Collaborating towards the neXt frontier

We are actively looking for potential collaborators to help push this community forward â€” especially **hospitals or medical institutions that can provide diverse, real-world clinical colonoscopy data (eg., data across different devices, modalities, patient populations, and clinical workflows)**. If youâ€™re interested in contributing or partnering with us, weâ€™d be very happy to connect.

Weâ€™re still on the journey toward building truly intelligent colonoscopy systems, and this project is very much under active development. We warmly welcome any feedback, ideas, or suggestions that can help shape its future.

For any inquiries or thoughts youâ€™d like to share, feel free to reach out to us at [ğŸ“®gepengai.ji@gmail.com](mailto:gepengai.ji@gmail.com)


## ğŸ’¬ Discussion Forum

This is just the start of building our Roman Empire ğŸ”±. Weâ€™re on a mission to make colonoscopies smarter, more accurate, and ultimately, save more lives. Want to join us on this exciting journey? Welcome to our [AI4Colonoscopy Discussion Forum](https://github.com/orgs/ai4colonoscopy/discussions/1)

  - [(SUBFORUM#1) ask any questions](https://github.com/orgs/ai4colonoscopy/discussions/categories/any-q-a) ğŸ˜¥ *â€œè®ºæ–‡ä¸­é‡è§äº†é—®é¢˜ï¼Ÿä»£ç ä¸ä¼šè·‘ï¼Ÿâ€œ*
  - [(SUBFORUM#2) showcase/promote your work](https://github.com/orgs/ai4colonoscopy/discussions/categories/show-tell) ğŸ˜¥ *â€æƒ³å¢åŠ è®ºæ–‡å½±å“åŠ›ï¼Ÿå¦‚ä½•å‘ç¤¾åŒºå®£ä¼ è‡ªå·±çš„å·¥ä½œï¼Ÿâ€œ*
  - [(SUBFORUM#3) access data resources](https://github.com/orgs/ai4colonoscopy/discussions/categories/data-helpdesk) ğŸ˜¥ *â€œä¸‹è½½ä¸åˆ°æ•°æ®ï¼Ÿå¦‚ä½•ä½¿ç”¨/å¤„ç†æ‰‹å¤´çš„æ•°æ®ï¼Ÿâ€*
  - [(SUBFORUM#4) share research ideas](https://github.com/orgs/ai4colonoscopy/discussions/categories/ideas-collaborations) ğŸ˜¥ *â€æ‰¾ä¸åˆ°åˆä½œè€…ï¼Ÿæƒ³ä¸å‡ºæœ‰è¶£çš„ideaï¼Ÿâ€œ*
